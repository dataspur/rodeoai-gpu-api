# Deduplication & Smart Triage Guide

## ðŸŽ¯ Overview

Your data ingestion pipeline now has **production-grade deduplication and smart triage**:

1. **Deduplication** - Never process the same data twice
2. **Smart Triage** - Reject irrelevant files in <1 second
3. **Quality Assessment** - Ensure data meets standards
4. **Review Queue** - Manual review for uncertain cases

---

## âœ… What Problems This Solves

### Before (Without Deduplication/Triage):
âŒ Upload `nfr_results_2024.csv` twice â†’ Process twice, waste GPU time
âŒ Upload `my_recipe_collection.csv` â†’ Process, extract nothing useful
âŒ Upload corrupt CSV â†’ Push bad data to database
âŒ Upload unclear format â†’ Push incomplete data
âŒ No way to know which files failed or need review

### After (With Deduplication/Triage):
âœ… Upload `nfr_results_2024.csv` twice â†’ **2nd upload rejected instantly**
âœ… Upload `my_recipe_collection.csv` â†’ **Rejected in <1 sec (irrelevant)**
âœ… Upload corrupt CSV â†’ **Rejected (poor quality), sent to review queue**
âœ… Upload unclear format â†’ **Sent to review queue for manual check**
âœ… `/review-queue` endpoint shows all files needing attention

---

## ðŸ”„ The 6-Step Processing Pipeline

Every uploaded file goes through this pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Check Duplicate File                          â”‚
â”‚  - Compute SHA-256 hash of file content                â”‚
â”‚  - Check if hash was seen before                       â”‚
â”‚  - If duplicate â†’ REJECT immediately                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“ (< 1ms)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Smart Triage (Relevance Check)                â”‚
â”‚  - Sample first 1000 bytes                             â”‚
â”‚  - Score based on rodeo keywords                       â”‚
â”‚  - If irrelevant â†’ REJECT, send to review queue       â”‚
â”‚  - Fast: ~100ms                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“ (If relevant or uncertain)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Extract Data (GPU Processing)                 â”‚
â”‚  - Parse CSV/Excel/PDF/Image                           â”‚
â”‚  - Extract events, riders, predictions, results        â”‚
â”‚  - Format to Lovable schema                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Check Duplicate Data (Semantic)               â”‚
â”‚  - Hash extracted data (not file)                      â”‚
â”‚  - Catches same data in different formats              â”‚
â”‚  - If duplicate â†’ REJECT                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“ (If unique data)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Assess Data Quality                           â”‚
â”‚  - Check for empty results                             â”‚
â”‚  - Validate critical fields present                    â”‚
â”‚  - Score data completeness (0-100)                     â”‚
â”‚  - If poor quality â†’ REJECT, send to review queue     â”‚
â”‚  - If uncertain â†’ Send to review queue (but process)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“ (If quality >= 60)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6: Auto-Push to Lovable                          â”‚
â”‚  - Push events, riders, predictions, results           â”‚
â”‚  - Data appears in rodeoai.app                         â”‚
â”‚  - SUCCESS!                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ” How Deduplication Works

### 1. Exact File Deduplication

**Detects:** Same file uploaded twice

```python
# SHA-256 hash of file content
file_hash = sha256(file_content)

if file_hash in seen_hashes:
    return "DUPLICATE FILE"
```

**Example:**
```bash
# First upload
python upload_local_data.py nfr_results_2024.csv
# âœ… Success

# Second upload (same file)
python upload_local_data.py nfr_results_2024.csv
# âŒ DUPLICATE: This file was already uploaded
```

### 2. Semantic Data Deduplication

**Detects:** Same data in different formats

```python
# Hash the actual data, not the file
data_hash = sha256(canonical_representation(extracted_data))

if data_hash in seen_hashes:
    return "DUPLICATE DATA"
```

**Example:**
```bash
# Upload CSV
python upload_local_data.py nfr_results.csv
# âœ… Success

# Convert to Excel and upload
python upload_local_data.py nfr_results.xlsx
# âŒ DUPLICATE DATA: This data was already uploaded (possibly in different format)
```

---

## ðŸŽ¯ How Smart Triage Works

### Keyword-Based Scoring

The system maintains two keyword lists:

#### Rodeo Keywords (Positive Score):
- bull, bronc, barrel, roping, wrestling, steer, calf
- prca, pbr, wpra, nfr, wnfr, rodeo
- las vegas, thomas mack, cowboy, arena
- score, time, seconds, points, rank, standings

#### Irrelevant Keywords (Negative Score):
- recipe, cooking, diet, fashion, weather
- stock market, finance, real estate, invoice
- medical, prescription, resume, cv

### Scoring Logic:

```python
score = 0
score += count(rodeo_keywords in text)
score -= count(irrelevant_keywords in text) * 2  # Weight negative more

if score >= 2:
    verdict = "relevant" â†’ PROCESS
elif score <= -2:
    verdict = "irrelevant" â†’ REJECT
else:
    verdict = "uncertain" â†’ REVIEW QUEUE
```

### Examples:

#### Example 1: Relevant File
```
Filename: nfr_round_7_bull_riding.csv
Content: "Stetson Wright, bull riding, 91.5 points, Las Vegas..."

Rodeo keywords: 5 matches (bull, riding, points, las vegas, wright)
Irrelevant keywords: 0
Score: +5
Verdict: RELEVANT â†’ PROCESS
```

#### Example 2: Irrelevant File
```
Filename: chocolate_cake_recipe.csv
Content: "ingredients, flour, sugar, chocolate, cooking time..."

Rodeo keywords: 0
Irrelevant keywords: 3 matches (recipe, cooking, ingredients)
Score: -6
Verdict: IRRELEVANT â†’ REJECT
```

#### Example 3: Uncertain File
```
Filename: data_2024.csv
Content: "name, location, date, score..."

Rodeo keywords: 1 match (score)
Irrelevant keywords: 0
Score: +1
Verdict: UNCERTAIN â†’ REVIEW QUEUE
```

---

## ðŸ“‹ Quality Assessment

After data extraction, quality is scored 0-100:

### Quality Score Deductions:

| Issue | Deduction |
|-------|-----------|
| No data extracted | -100 (instant reject) |
| Very few records (< 5) | -30 |
| Missing event names | -20 |
| Missing rider names | -20 |
| Unclear structure | -25 |
| Low extraction confidence | -15 |

### Quality Verdicts:

| Score | Verdict | Action |
|-------|---------|--------|
| 80-100 | Excellent | PROCESS |
| 60-79 | Good | PROCESS |
| 40-59 | Fair | REVIEW QUEUE |
| 20-39 | Poor | REVIEW QUEUE |
| 0-19 | Very Poor | REJECT |

---

## ðŸ—‚ï¸ Review Queue

Files sent to review queue can be viewed:

```bash
# View review queue
curl -H "x-api-key: YOUR_KEY" \
  https://YOUR-RUNPOD-URL/review-queue
```

**Response:**
```json
{
  "status": "success",
  "queue_length": 3,
  "items": [
    {
      "filename": "unclear_format.csv",
      "reason": "Uncertain quality or relevance",
      "file_hash": "abc123...",
      "assessment": {
        "verdict": "uncertain",
        "confidence": 45,
        "quality_score": 55
      },
      "added_at": "2024-12-04T...",
      "status": "pending_review"
    },
    {
      "filename": "recipe_book.csv",
      "reason": "File appears irrelevant",
      "assessment": {
        "verdict": "irrelevant",
        "reasons": ["Contains cooking/recipe keywords"]
      }
    }
  ]
}
```

---

## ðŸŽ›ï¸ Override Options

### Skip Deduplication (Force Re-upload):

```bash
python upload_local_data.py --skip-dedup nfr_results.csv
```

Or via API:
```bash
curl -X POST "https://YOUR-RUNPOD-URL/ingest-historical-data" \
  -F "file=@data.csv" \
  -F "skip_deduplication=true"
```

**Use case:** You updated the file and want to re-upload

### Skip Triage (Force Process Everything):

```bash
python upload_local_data.py --skip-triage data.csv
```

Or via API:
```bash
curl -X POST "https://YOUR-RUNPOD-URL/ingest-historical-data" \
  -F "file=@data.csv" \
  -F "skip_triage=true"
```

**Use case:** You know the file is relevant but system might flag it

---

## ðŸ“Š Enhanced Response Structure

Every upload now returns comprehensive status:

```json
{
  "status": "success",  // or "duplicate", "rejected", "needs_review"
  "filename": "nfr_2024.csv",
  "file_size": 145320,

  "deduplication": {
    "file_duplicate": false,
    "data_duplicate": false
  },

  "triage": {
    "verdict": "relevant",
    "confidence": 95,
    "reasons": [
      "Filename contains rodeo keywords",
      "Content sample contains rodeo keywords",
      "Data file format (CSV/Excel)"
    ]
  },

  "quality": {
    "verdict": "excellent",
    "score": 95,
    "issues": [],
    "warnings": []
  },

  "processed_data": {
    "events_count": 8,
    "riders_count": 45,
    "predictions_count": 0,
    "results_count": 120
  },

  "action_taken": "process",  // or "review", "reject"
  "review_queue_id": null,  // or queue ID if in review

  "push_results": [
    {"type": "result", "status": "success", "id": "..."},
    ...
  ]
}
```

---

## ðŸ’¡ Real-World Scenarios

### Scenario 1: Bulk Historical Upload

You're uploading 200 historical CSV files:

```bash
python upload_local_data.py --batch ~/rodeo_data/historical/
```

**Result:**
- 180 files: âœ… Processed successfully
- 5 files: âš ï¸  Duplicates (rejected)
- 10 files: âš ï¸  Uncertain quality (review queue)
- 5 files: âŒ Irrelevant (rejected)

**You only need to manually review 10 files** instead of all 200!

### Scenario 2: Mixed Quality Folder

Your data folder has:
- Rodeo results CSVs âœ…
- Personal notes TXT files âŒ
- Recipe spreadsheets âŒ
- Some corrupt files âŒ

```bash
python upload_local_data.py --batch ~/messy_folder/
```

**Smart triage automatically:**
- Processes only rodeo CSVs
- Rejects recipes and notes instantly
- Flags corrupt files for review
- Saves hours of manual sorting!

### Scenario 3: Re-upload After Fixing

You uploaded a file with issues, fixed it, and want to re-upload:

```bash
# First upload (has issues)
python upload_local_data.py data.csv
# â†’ Sent to review queue

# Fix the file
# ...edit data.csv...

# Re-upload with dedup skip
python upload_local_data.py --skip-dedup data.csv
# â†’ Processes the fixed version
```

---

## ðŸš€ Performance Benefits

### Without Deduplication/Triage:
- âŒ Process 200 files: ~30 minutes GPU time
- âŒ 15 duplicates: Wasted 4.5 minutes
- âŒ 20 irrelevant files: Wasted 6 minutes
- âŒ Total wasted: 10.5 minutes + database bloat

### With Deduplication/Triage:
- âœ… Reject 15 duplicates: <1 second total
- âœ… Reject 20 irrelevant: <20 seconds total
- âœ… Process only 165 relevant files: ~24.75 minutes
- âœ… Time saved: 10.5 minutes (35% faster!)
- âœ… Database stays clean

---

## ðŸ”§ Customization

### Add Your Own Keywords:

Edit `deduplication.py`:

```python
# Add more rodeo keywords
self.rodeo_keywords = {
    'bull', 'bronc', 'barrel',
    # Add your custom keywords:
    'rope', 'chute', 'bucking', 'spurring', ...
}

# Add more irrelevant keywords
self.irrelevant_keywords = {
    'recipe', 'cooking',
    # Add your custom exclusions:
    'unrelated_keyword', ...
}
```

### Adjust Quality Thresholds:

```python
# In assess_data_quality():
if total_records < 5:  # Change threshold
    quality_score -= 30
```

---

## ðŸ“ˆ Monitoring

### Check Review Queue Regularly:

```bash
curl -H "x-api-key: YOUR_KEY" \
  https://YOUR-RUNPOD-URL/review-queue | jq
```

### Watch Logs:

```
INFO: Triage verdict: relevant (score: 5)
INFO: Quality verdict: excellent (score: 95)
WARNING: DUPLICATE FILE DETECTED: nfr_2024.csv
WARNING: IRRELEVANT FILE: recipes.csv
INFO: NEEDS REVIEW: unclear_data.csv
```

---

## âœ… Best Practices

1. **Run with defaults first** - Let deduplication and triage work
2. **Check review queue daily** - Review flagged files
3. **Use skip flags sparingly** - Only when you're certain
4. **Monitor rejection reasons** - Adjust keywords if needed
5. **Keep backups** - Always keep original files

---

## ðŸŽ¯ Summary

You now have **production-grade data ingestion**:

âœ… **No duplicate processing** - Saves GPU time and money
âœ… **Fast irrelevance detection** - Rejects junk in <1 second
âœ… **Quality guaranteed** - Only good data reaches database
âœ… **Manual review for edge cases** - Human oversight when needed
âœ… **Comprehensive logging** - Full transparency
âœ… **Override options** - Flexibility when needed

**Result:** Clean, efficient, professional data pipeline! ðŸš€